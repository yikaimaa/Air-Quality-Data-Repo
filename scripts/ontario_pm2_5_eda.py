# -*- coding: utf-8 -*-
"""ontario PM2.5 EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G43wEGWFjHJNduGztWxH9b88mHJHTgWM

# Case Study 1: Ontario PM2.5 Predication & Altering System

## 1 Data Preprocessing - Ontario PM2.5 Data

### 1.1 Combine annual data into one single table
"""

import re
import numpy as np
import pandas as pd
from io import StringIO
import requests

BASE_RAW = "https://raw.githubusercontent.com/yikaimaa/Air-Quality-Data-Repo/main/Datasets/Ontario/PM25/"

files = [
    "ON_PM25_2020-01-01_2020-12-31.csv",
    "ON_PM25_2021-01-01_2021-12-31.csv",
    "ON_PM25_2022-01-01_2022-12-31.csv",
    "ON_PM25_2023-01-01_2023-12-31.csv",
    "ON_PM25_2024-01-01_2024-12-31.csv",
]

def parse_ontario_pm25_text(text: str, year: int) -> pd.DataFrame:
    lines = text.splitlines()

    chunks = []
    cols = None
    buf = []
    in_data = False

    for line in lines:
        if line.startswith("Station ID,Pollutant,Date"):
            cols = [c for c in line.split(",") if c != ""]
            in_data = True
            buf = []
            continue

        if not in_data:
            continue

        if re.match(r"^\d{3,},", line):
            buf.append(line.rstrip().rstrip(","))
            continue

        if buf and cols:
            df_chunk = pd.read_csv(StringIO("\n".join(buf)), header=None, names=cols, engine="python")
            chunks.append(df_chunk)
        buf = []
        in_data = False

    if in_data and buf and cols:
        df_chunk = pd.read_csv(StringIO("\n".join(buf)), header=None, names=cols, engine="python")
        chunks.append(df_chunk)

    if not chunks:
        return pd.DataFrame()

    df = pd.concat(chunks, ignore_index=True)

    hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", c)]
    df[hour_cols] = df[hour_cols].apply(pd.to_numeric, errors="coerce").replace(9999, np.nan)

    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")

    df["year"] = year
    return df

dfs = []
for f in files:
    year = int(re.search(r"ON_PM25_(\d{4})-", f).group(1))
    url = BASE_RAW + f

    r = requests.get(url, timeout=60)
    r.raise_for_status()

    df_year = parse_ontario_pm25_text(r.text, year=year)
    dfs.append(df_year)

pm25_all = pd.concat(dfs, ignore_index=True)

print(pm25_all.shape)
pm25_all.head()

"""### 1.2 Extract station information from raw data"""

import re
import pandas as pd
import requests

BASE_RAW = "https://raw.githubusercontent.com/yikaimaa/Air-Quality-Data-Repo/main/Datasets/Ontario/PM25/"

files = [
    "ON_PM25_2020-01-01_2020-12-31.csv",
    "ON_PM25_2021-01-01_2021-12-31.csv",
    "ON_PM25_2022-01-01_2022-12-31.csv",
    "ON_PM25_2023-01-01_2023-12-31.csv",
    "ON_PM25_2024-01-01_2024-12-31.csv",
]

STATION_RE = re.compile(
    r"Station,\s*([^,]*?)\s*\((\d+)\).*?"
    r"Latitude,\s*([-\d.]+)\s*Longitude,\s*([-\d.]+)",
    flags=re.IGNORECASE | re.DOTALL
)

def extract_station_meta(text: str) -> pd.DataFrame:
    records = []
    for m in STATION_RE.finditer(text):
        records.append({
            "Station ID": int(m.group(2)),
            "station_name": m.group(1).strip(),
            "latitude": float(m.group(3)),
            "longitude": float(m.group(4)),
        })

    return pd.DataFrame(records, columns=["Station ID", "station_name", "latitude", "longitude"])

meta_dfs = []
for f in files:
    url = BASE_RAW + f
    r = requests.get(url, timeout=60)
    r.raise_for_status()
    meta_dfs.append(extract_station_meta(r.text))

station_lookup = pd.concat(meta_dfs, ignore_index=True)

station_lookup = (
    station_lookup
    .sort_values(["Station ID"])
    .drop_duplicates(subset=["Station ID"], keep="first")
    .reset_index(drop=True)
)

print("station_lookup rows:", len(station_lookup))
station_lookup.head()

"""### 1.3 Map station info to the single table"""

pm25_all["Station ID"] = pd.to_numeric(pm25_all["Station ID"], errors="coerce").astype("Int64")
station_lookup["Station ID"] = pd.to_numeric(station_lookup["Station ID"], errors="coerce").astype("Int64")

pm25_hourly_wide = pm25_all.merge(station_lookup, on="Station ID", how="left")

print(pm25_hourly_wide.shape)
pm25_hourly_wide.head()

"""### 1.4 Remove Duplicates

#### 1.4.1 Remove Fully Duplicated Rows
"""

full_dup_extra = pm25_hourly_wide.duplicated(keep="first").sum()
print("extra fully duplicated rows BEFORE:", full_dup_extra)

pm25_hourly_wide_nofulldup = pm25_hourly_wide.drop_duplicates(keep="first").reset_index(drop=True)

full_dup_extra_after = pm25_hourly_wide_nofulldup.duplicated(keep="first").sum()
print("extra fully duplicated rows AFTER:", full_dup_extra_after)

print("shape BEFORE:", pm25_hourly_wide.shape)
print("shape AFTER :", pm25_hourly_wide_nofulldup.shape)

"""#### 1.4.2 Merge Duplicated Key Rows"""

df = pm25_hourly_wide_nofulldup.copy()

key = ["Station ID", "Pollutant", "Date"]

n_dup_keys  = df.groupby(key).size().gt(1).sum()
n_dup_rows  = df.duplicated(subset=key, keep=False).sum()
n_dup_extra = df.duplicated(subset=key, keep="first").sum()

print("number of duplicated keys:", n_dup_keys)
print("rows belonging to duplicated keys (keep=False):", n_dup_rows)
print("extra rows due to key duplicates (keep=first):", n_dup_extra)

key_dup_rows_df = (
    df[df.duplicated(subset=key, keep=False)]
      .sort_values(key)
      .reset_index(drop=True)
)

key_dup_rows_df.head(50)

import re
import numpy as np
import pandas as pd

if "pm25_hourly_wide_nofulldup" in globals():
    df0 = pm25_hourly_wide_nofulldup.copy()
elif "pm25_hourly_wide" in globals():
    df0 = pm25_hourly_wide.drop_duplicates().reset_index(drop=True)
else:
    raise NameError("fail to find pm25_hourly_wide or pm25_hourly_wide_nofulldup, please use pm25_hourly_wide")

key = ["Station ID", "Pollutant", "Date"]
hour_cols = [c for c in df0.columns if re.fullmatch(r"H\d{2}", c)]

df0[hour_cols] = df0[hour_cols].replace([-999, 9999], np.nan)

dup_mask = df0.duplicated(subset=key, keep=False)
df_dup = df0.loc[dup_mask].copy()
df_solo = df0.loc[~dup_mask].copy()

def is_conflict_group(g: pd.DataFrame) -> bool:
    nunq = g[hour_cols].apply(lambda s: s.dropna().nunique(), axis=0)
    return (nunq > 1).any()

conflict_flag = (
    df_dup.groupby(key, group_keys=False)
          .apply(is_conflict_group)
          .reset_index(name="is_conflict")
)

conflict_keys = conflict_flag.loc[conflict_flag["is_conflict"], key].drop_duplicates()
non_conflict_keys = conflict_flag.loc[~conflict_flag["is_conflict"], key].drop_duplicates()

print("duplicated keys total:", len(conflict_flag))
print("conflict keys:", len(conflict_keys))
print("non-conflict (complement-only) keys:", len(non_conflict_keys))

def merge_no_conflict_group(g: pd.DataFrame) -> pd.Series:
    gg = g.copy()
    valid_cnt = gg[hour_cols].notna().sum(axis=1)
    gg = gg.loc[valid_cnt.sort_values(ascending=False).index]

    base = gg.iloc[0].copy()
    for i in range(1, len(gg)):
        row = gg.iloc[i]
        missing = base[hour_cols].isna() & row[hour_cols].notna()
        idx = missing.index[missing]
        base.loc[idx] = row.loc[idx]

    return base

df_dup_non_conflict = df_dup.merge(non_conflict_keys, on=key, how="inner")
merged_non_conflict = (
    df_dup_non_conflict.groupby(key, group_keys=False)
                       .apply(merge_no_conflict_group)
                       .reset_index(drop=True)
)

def merge_conflict_group(g: pd.DataFrame) -> pd.Series:
    gg = g.copy()
    valid_cnt = gg[hour_cols].notna().sum(axis=1)
    gg = gg.loc[valid_cnt.sort_values(ascending=False).index]
    out = gg.iloc[0].copy()

    meta_cols = [c for c in gg.columns if c not in hour_cols]
    for c in meta_cols:
        if pd.isna(out[c]):
            non_na = gg[c].dropna()
            if len(non_na) > 0:
                out[c] = non_na.iloc[0]

    for h in hour_cols:
        vals = gg[h].dropna()
        if vals.empty:
            out[h] = np.nan
            continue

        uniq = pd.unique(vals)
        if len(uniq) == 1:
            out[h] = uniq[0]
        else:
            uniq_num = pd.to_numeric(pd.Series(uniq), errors="coerce").dropna().values
            out[h] = float(np.mean(uniq_num)) if len(uniq_num) else np.nan

    return out

df_dup_conflict = df_dup.merge(conflict_keys, on=key, how="inner")
merged_conflict = (
    df_dup_conflict.groupby(key, group_keys=False)
                   .apply(merge_conflict_group)
                   .reset_index(drop=True)
)

pm25_hourly_wide_final = (
    pd.concat([df_solo, merged_non_conflict, merged_conflict], ignore_index=True)
      .sort_values(key)
      .reset_index(drop=True)
)

print("final shape:", pm25_hourly_wide_final.shape)
print("remaining key-duplicated rows:",
      pm25_hourly_wide_final.duplicated(subset=key, keep=False).sum())

pm25_hourly_wide_final.head()

"""#### 1.4.3 Validation"""

key = ["Station ID", "Pollutant", "Date"]

df_check = pm25_hourly_wide_final

dup_rows  = df_check.duplicated(subset=key, keep=False).sum()
dup_extra = df_check.duplicated(subset=key, keep="first").sum()
dup_keys  = (df_check.groupby(key).size() > 1).sum()

print("key-duplicated rows (keep=False):", dup_rows)
print("extra rows due to key duplicates (keep=first):", dup_extra)
print("number of duplicated keys:", dup_keys)

"""## 2 EDA - Ontario PM2.5 Data

### 2.1 Missing Value Profilling
"""

import re
import numpy as np
import pandas as pd

df = pm25_hourly_wide_final.copy()

key = ["Station ID", "Pollutant", "Date"]
hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", c)]
meta_cols = ["station_name", "latitude", "longitude"]

df["Date"] = pd.to_datetime(df["Date"]).dt.normalize()

station_meta = (
    df[["Station ID"] + meta_cols]
    .drop_duplicates(subset=["Station ID"])
    .reset_index(drop=True)
)

all_dates = pd.date_range("2020-01-01", "2024-12-31", freq="D")

pairs = df[["Station ID", "Pollutant"]].drop_duplicates().reset_index(drop=True)
full = (
    pairs.assign(_k=1)
         .merge(pd.DataFrame({"Date": all_dates, "_k": 1}), on="_k")
         .drop(columns="_k")
)

pm25_hourly_wide_panel = full.merge(df, on=key, how="left")

pm25_hourly_wide_panel = (
    pm25_hourly_wide_panel
    .drop(columns=meta_cols, errors="ignore")
    .merge(station_meta, on="Station ID", how="left")
)

if "year" in pm25_hourly_wide_panel.columns:
    pm25_hourly_wide_panel["year"] = pm25_hourly_wide_panel["year"].fillna(pm25_hourly_wide_panel["Date"].dt.year)
else:
    pm25_hourly_wide_panel["year"] = pm25_hourly_wide_panel["Date"].dt.year
pm25_hourly_wide_panel["year"] = pm25_hourly_wide_panel["year"].astype(int)

ordered_cols = key + ["year"] + meta_cols + hour_cols
pm25_hourly_wide_panel = pm25_hourly_wide_panel[[c for c in ordered_cols if c in pm25_hourly_wide_panel.columns]]

print("before:", df.shape, "after:", pm25_hourly_wide_panel.shape)
pm25_hourly_wide_panel.head()

"""#### 2.1.1 Missing Value by Column"""

import pandas as pd
import numpy as np

df = pm25_hourly_wide_final.copy()

n = len(df)

missing_by_col = (
    pd.DataFrame({
        "column": df.columns,
        "total_count": n,
        "dtype": df.dtypes.astype(str).values,
        "missing_count": df.isna().sum().values,
        "missing_rate": df.isna().mean().values  # 0~1
    })
    .sort_values("missing_rate", ascending=False)
    .reset_index(drop=True)
)

missing_by_col

"""#### 2.1.2 Missing Value by Station"""

import pandas as pd
import numpy as np

df = pm25_hourly_wide_panel.copy()

n = len(df)

missing_by_col = (
    pd.DataFrame({
        "column": df.columns,
        "total_count": [n] * df.shape[1],
        "dtype": df.dtypes.astype(str).values,
        "missing_count": df.isna().sum().values,
        "missing_rate": df.isna().mean().values  # 0~1
    })
    .sort_values("missing_rate", ascending=False)
    .reset_index(drop=True)
)

missing_by_col

import re
import numpy as np
import pandas as pd

df = pm25_hourly_wide_panel.copy()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", c)]
n_hours = len(hour_cols)

df2 = df.assign(
    missing_hours=df[hour_cols].isna().sum(axis=1),
    any_missing=lambda x: x["missing_hours"] > 0,
    all_missing=lambda x: x["missing_hours"] == n_hours
)

station_missing = (
    df2.groupby(["Station ID", "Pollutant"], as_index=False)
       .agg(
           total_days=("Date", "size"),
           missing_cells=("missing_hours", "sum"),
           avg_missing_hours_per_day=("missing_hours", "mean"),
           days_with_any_missing=("any_missing", "sum"),
           pct_days_with_any_missing=("any_missing", "mean"),
           days_all_missing=("all_missing", "sum"),
           pct_days_all_missing=("all_missing", "mean"),
           station_name=("station_name", "first"),
           latitude=("latitude", "first"),
           longitude=("longitude", "first"),
       )
)

station_missing["total_cells"] = station_missing["total_days"] * n_hours
station_missing["missing_rate_cells"] = station_missing["missing_cells"] / station_missing["total_cells"]

station_missing = station_missing.sort_values("missing_rate_cells", ascending=False).reset_index(drop=True)

rate_cols = ["missing_rate_cells", "pct_days_with_any_missing", "pct_days_all_missing"]
station_missing[rate_cols] = station_missing[rate_cols].round(6)

station_missing

"""#### 2.1.3 Missing Value (All-day) Over Time"""

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ====== INPUT: pick the best available dataframe ======
if "pm25_hourly_wide_panel" in globals():
    df = pm25_hourly_wide_panel.copy()
elif "pm25_hourly_wide_final" in globals():
    df = pm25_hourly_wide_final.copy()
elif "pm25_hourly_final" in globals():
    df = pm25_hourly_final.copy()
else:
    raise NameError("could not find pm25_hourly_wide_panel / pm25_hourly_wide_final / pm25_hourly_final")

# Ensure Date is normalized to date-only
df["Date"] = pd.to_datetime(df["Date"]).dt.normalize()

# Identify hour columns
hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", str(c))]

if len(hour_cols) != 24:
    print(f"Warning: detected {len(hour_cols)} hour columns (expected 24). Columns: {hour_cols}")

# All-day missing: all 24 hours are NaN
all_day_missing = df[hour_cols].isna().all(axis=1)

# Province-level time series: count of all-day-missing station-days per date
ts_all_day_missing_cnt = (
    df.assign(all_day_missing=all_day_missing)
      .groupby("Date", as_index=False)["all_day_missing"]
      .sum()
      .rename(columns={"all_day_missing": "all_day_missing_stationdays"})
      .sort_values("Date")
      .reset_index(drop=True)
)

# Basic context: number of station-pollutant pairs (i.e., expected station-days per date in a balanced panel)
n_pairs = df[["Station ID", "Pollutant"]].drop_duplicates().shape[0]
ts_all_day_missing_cnt["expected_stationdays_per_date"] = n_pairs

# Display head
ts_all_day_missing_cnt.head()

# Plot (single line chart)
plt.figure(figsize=(12, 4))
plt.plot(ts_all_day_missing_cnt["Date"], ts_all_day_missing_cnt["all_day_missing_stationdays"])
plt.title("Ontario PM2.5: All-day-missing station-days over time (2020–2024)")
plt.xlabel("Date")
plt.ylabel("Count of station-days with all 24 hours missing")
plt.tight_layout()
plt.show()

"""### 2.2 Target (PM2.5) Analysis"""

import re
import numpy as np
import pandas as pd

df = pm25_hourly_wide_panel.copy()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", str(c))]
hour_cols = sorted(hour_cols)

num_cols = hour_cols

def col_profile(s: pd.Series) -> pd.Series:
    x = pd.to_numeric(s, errors="coerce").dropna()
    if len(x) == 0:
        return pd.Series({
            "min": np.nan,
            "p05": np.nan,
            "p25": np.nan,
            "median": np.nan,
            "mean": np.nan,
            "p75": np.nan,
            "p95": np.nan,
            "max": np.nan,
            "std": np.nan,
            "skew": np.nan,
            "kurtosis": np.nan,
        })
    return pd.Series({
        "min": float(x.min()),
        "p05": float(x.quantile(0.05)),
        "p25": float(x.quantile(0.25)),
        "median": float(x.median()),
        "mean": float(x.mean()),
        "p75": float(x.quantile(0.75)),
        "p95": float(x.quantile(0.95)),
        "max": float(x.max()),
        "std": float(x.std()),
        "skew": float(x.skew()),
        "kurtosis": float(x.kurtosis()),
    })

col_stats = (
    df[num_cols]
      .apply(col_profile, axis=0)
      .T
      .reset_index()
      .rename(columns={"index": "column"})
      .sort_values("column")
      .reset_index(drop=True)
)

col_stats

"""#### 2.2.1 PM2.5 Data Distribtuion"""

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pm25_hourly_wide_panel.copy()

if "Pollutant" in df.columns:
    df = df[df["Pollutant"].astype(str).str.contains("Fine Particulate", case=False, na=False)].copy()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", str(c))]
df["Date"] = pd.to_datetime(df["Date"]).dt.normalize()

df["pm25_24h_mean_station"] = df[hour_cols].mean(axis=1, skipna=True)

ontario_daily = (
    df.groupby("Date")["pm25_24h_mean_station"]
      .mean()
      .rename("ontario_daily_mean")
      .reset_index()
      .sort_values("Date")
      .reset_index(drop=True)
)

s = ontario_daily["ontario_daily_mean"].dropna()

summary = pd.DataFrame({
    "metric": ["n_days", "mean", "std", "min", "p50", "p90", "p95", "p99", "max"],
    "value": [
        len(s),
        s.mean(),
        s.std(),
        s.min(),
        s.quantile(0.50),
        s.quantile(0.90),
        s.quantile(0.95),
        s.quantile(0.99),
        s.max()
    ]
})
summary

plt.figure(figsize=(10, 4))
plt.hist(s, bins=120)
plt.yscale("log")
plt.xlabel("Ontario daily mean PM2.5 (µg/m³)")
plt.ylabel("Count (log scale)")
plt.title("Distribution of Ontario Daily Mean PM2.5 (2020–2024)")
plt.tight_layout()
plt.show()

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", str(c))]

if "col_stats" in globals():
    cs = col_stats.copy()
    cs = cs[cs["column"].isin(hour_cols)].copy()
    cs["std"] = pd.to_numeric(cs["std"], errors="coerce")
    cs = cs.dropna(subset=["std"])
else:
    cs = pd.DataFrame({
        "column": hour_cols,
        "std": df[hour_cols].apply(pd.to_numeric, errors="coerce").std(skipna=True).values
    }).dropna(subset=["std"])

col_max = cs.loc[cs["std"].idxmax(), "column"]
col_min = cs.loc[cs["std"].idxmin(), "column"]

std_max = float(cs.loc[cs["std"].idxmax(), "std"])
std_min = float(cs.loc[cs["std"].idxmin(), "std"])

print(f"STD MAX COL: {col_max}, std={std_max:.6f}")
print(f"STD MIN COL: {col_min}, std={std_min:.6f}")

def plot_hist(series: pd.Series, title: str, bins: int = 120, log_y: bool = True):
    s = pd.to_numeric(series, errors="coerce").dropna()
    plt.figure(figsize=(10, 4))
    plt.hist(s, bins=bins)
    if log_y:
        plt.yscale("log")
    plt.xlabel("PM2.5 (µg/m³)")
    plt.ylabel("Count" + (" (log scale)" if log_y else ""))
    plt.title(title)
    plt.tight_layout()
    plt.show()

plot_hist(df[col_max], f"Distribution of {col_max} (std max = {std_max:.3f})", log_y=True)
plot_hist(df[col_min], f"Distribution of {col_min} (std min = {std_min:.3f})", log_y=True)

"""#### 2.2.2 PM2.5 by Time"""

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pm25_hourly_wide_panel.copy()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", c)]
df["Date"] = pd.to_datetime(df["Date"]).dt.normalize()

df["pm25_24h_mean"] = df[hour_cols].mean(axis=1, skipna=True)

df["month"] = df["Date"].dt.to_period("M").dt.to_timestamp()

station_month = (
    df.groupby(["Station ID", "month"])["pm25_24h_mean"]
      .mean()
      .reset_index()
)

monthly_pm25_station_weighted = (
    station_month.groupby("month")["pm25_24h_mean"]
      .mean()
      .reset_index()
      .rename(columns={"pm25_24h_mean": "station_weighted_mean"})
      .sort_values("month")
      .reset_index(drop=True)
)

monthly_pm25_station_weighted

plt.figure(figsize=(12, 5))
plt.plot(
    monthly_pm25_station_weighted["month"],
    monthly_pm25_station_weighted["station_weighted_mean"],
    marker="o"
)
plt.title("Ontario PM2.5 by Month (Station-weighted mean)")
plt.xlabel("Month")
plt.ylabel("PM2.5 (µg/m³)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**We notice the high PM2.5 level on 2023 June. It was caused by wildfire after investigation.**

#### 2.2.3 PM2.5 by Hour
"""

import re
import pandas as pd
import matplotlib.pyplot as plt

df = pm25_hourly_wide_panel.copy()

hour_cols = sorted([c for c in df.columns if re.fullmatch(r"H\d{2}", str(c))])

hourly_mean = df[hour_cols].mean(axis=0)

hours = [int(c[1:]) for c in hour_cols]

plt.figure()
plt.plot(hours, hourly_mean.values, marker="o")
plt.xticks(hours)
plt.xlabel("Hour (1–24)")
plt.ylabel("Mean PM2.5")
plt.title("Mean PM2.5 by Hour (H01–H24)")
plt.grid(True, alpha=0.3)
plt.show()

"""#### 2.2.4 PM2.5 by Station"""

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pm25_hourly_wide_panel.copy()

if ("station_name" not in df.columns) and ("station_lookup" in globals()):
    df = df.merge(station_lookup, on="Station ID", how="left")

df = df[df["Pollutant"].astype(str).str.contains("Fine Particulate", case=False, na=False)].copy()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", str(c))]
hour_cols = sorted(hour_cols, key=lambda x: int(str(x)[1:]))

if len(hour_cols) == 0:
    raise ValueError("could not find H01~H24")

df["pm25_daily_mean"] = df[hour_cols].mean(axis=1)

group_cols = ["Station ID"]
if "station_name" in df.columns:
    group_cols.append("station_name")

station_mean = (
    df.groupby(group_cols, as_index=False)
      .agg(
          mean_pm25=("pm25_daily_mean", "mean"),
          n_days_non_missing=("pm25_daily_mean", "count")
      )
      .sort_values("mean_pm25", ascending=False)
      .reset_index(drop=True)
)

if "station_name" in station_mean.columns:
    station_mean["station_label"] = (
        station_mean["station_name"].fillna("Unknown").astype(str)
        + " (" + station_mean["Station ID"].astype(str) + ")"
    )
else:
    station_mean["station_label"] = station_mean["Station ID"].astype(str)

display(station_mean.head(10))

plt.figure(figsize=(14, 6))
plt.bar(station_mean["station_label"], station_mean["mean_pm25"])
plt.xticks(rotation=60, ha="right")
plt.xlabel("Station")
plt.ylabel("Mean daily PM2.5 (µg/m³)")
plt.title("Mean PM2.5 by Station (Daily Mean Averaged Over 2020–2024)")
plt.tight_layout()
plt.show()

!pip -q install geopandas

import re
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

df = pm25_hourly_wide_panel.copy()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", c)]
df["Date"] = pd.to_datetime(df["Date"]).dt.normalize()

df["pm25_24h_mean"] = df[hour_cols].mean(axis=1, skipna=True)

station_pm25 = (
    df.groupby("Station ID", as_index=False)
      .agg(
          mean_pm25=("pm25_24h_mean", "mean"),
          station_name=("station_name", "first"),
          latitude=("latitude", "first"),
          longitude=("longitude", "first")
      )
      .dropna(subset=["latitude", "longitude"])
)

v = station_pm25["mean_pm25"].astype(float)

q_low, q_high = v.quantile(0.05), v.quantile(0.95)
v_clip = v.clip(q_low, q_high)

v_norm = (v_clip - v_clip.min()) / (v_clip.max() - v_clip.min() + 1e-12)

gamma = 3.0

base_size = 30
scale_size = 2000
station_pm25["marker_size"] = base_size + scale_size * (v_norm ** gamma)

gdf_pts = gpd.GeoDataFrame(
    station_pm25,
    geometry=gpd.points_from_xy(station_pm25["longitude"], station_pm25["latitude"]),
    crs="EPSG:4326"
)

ne_admin1_url = "https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_1_states_provinces.zip"
admin1 = gpd.read_file(ne_admin1_url)

ont = admin1[
    (admin1.get("admin").astype(str).str.lower() == "canada") &
    (admin1.get("name").astype(str).str.lower() == "ontario")
].copy()

if ont.empty:
    print("Ontario not found. Columns:", list(admin1.columns))

ont_3857 = ont.to_crs(epsg=3857)
pts_3857 = gdf_pts.to_crs(epsg=3857)

plt.figure(figsize=(10, 10))
ax = plt.gca()

ont_3857.plot(ax=ax, edgecolor="black", facecolor="none", linewidth=1)

pts_3857.plot(
    ax=ax,
    markersize=pts_3857["marker_size"],
    alpha=0.6,
)

ax.set_title(f"Ontario PM2.5 Stations (circle size ~ mean PM2.5, gamma={gamma})")
ax.set_axis_off()
plt.tight_layout()
plt.show()

station_pm25.sort_values("mean_pm25", ascending=False).head(10)

"""#### 2.2.5 PM2.5 by Month"""

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pm25_hourly_wide_panel.copy()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", c)]
df["Date"] = pd.to_datetime(df["Date"]).dt.normalize()

df["pm25_24h_mean"] = df[hour_cols].mean(axis=1, skipna=True)

df["month"] = df["Date"].dt.month

station_month = (
    df.groupby(["Station ID", "month"])["pm25_24h_mean"]
      .mean()
      .reset_index()
)

month_of_year = (
    station_month.groupby("month")["pm25_24h_mean"]
      .mean()
      .reset_index()
      .rename(columns={"pm25_24h_mean": "pm25_month_mean_station_weighted"})
      .sort_values("month")
      .reset_index(drop=True)
)

month_of_year

plt.figure(figsize=(10, 4))
plt.bar(month_of_year["month"], month_of_year["pm25_month_mean_station_weighted"])
plt.xticks(range(1, 13))
plt.title("Ontario PM2.5 Seasonality (Month-of-Year, Station-weighted, 2020–2024)")
plt.xlabel("Month (1–12)")
plt.ylabel("PM2.5 (µg/m³)")
plt.tight_layout()
plt.show()

"""### 2.3 Station-level Episode Timeline"""

import re
import numpy as np
import pandas as pd

df = pm25_hourly_wide_panel.copy()

df["Date"] = pd.to_datetime(df["Date"]).dt.normalize()

hour_cols = [c for c in df.columns if re.fullmatch(r"H\d{2}", c)]

df["valid_hours"] = df[hour_cols].notna().sum(axis=1)
df["pm25_24h_mean"] = df[hour_cols].mean(axis=1, skipna=True)

THRESH = 25
MIN_HOURS = 18
df["event_day"] = (df["pm25_24h_mean"] >= THRESH) & (df["valid_hours"] >= MIN_HOURS)

df = df.sort_values(["Station ID", "Date"]).reset_index(drop=True)

prev_date = df.groupby("Station ID")["Date"].shift(1)
prev_event = df.groupby("Station ID")["event_day"].shift(1)

new_block = (
    df["event_day"] &
    (
        (prev_event != True) |
        ((df["Date"] - prev_date).dt.days != 1)
    )
)

df["episode_id"] = np.where(df["event_day"], new_block.groupby(df["Station ID"]).cumsum(), np.nan)

episode_summary = (
    df[df["event_day"]]
    .groupby(["Station ID", "episode_id"], as_index=False)
    .agg(
        start=("Date", "min"),
        end=("Date", "max"),
        duration_days=("Date", "nunique"),
        mean_pm25=("pm25_24h_mean", "mean"),
        peak_pm25=("pm25_24h_mean", "max"),
        station_name=("station_name", "first"),
        latitude=("latitude", "first"),
        longitude=("longitude", "first"),
    )
)

MIN_LEN = 2
episode_summary = episode_summary[episode_summary["duration_days"] >= MIN_LEN].reset_index(drop=True)

print("episodes:", len(episode_summary))
episode_summary.head()

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

ep = episode_summary.copy()

ep["start"] = pd.to_datetime(ep["start"])
ep["end"]   = pd.to_datetime(ep["end"])

station_ep_cnt = ep.groupby("Station ID").size().sort_values(ascending=False)
stations_ordered = station_ep_cnt.index.tolist()

station_label_map = (
    ep.drop_duplicates("Station ID")
      .set_index("Station ID")
      .apply(lambda r: f"{r['station_name']} ({int(r.name)})", axis=1)
      .to_dict()
)

fig_height = max(6, 0.35 * len(stations_ordered) + 2)
fig, ax = plt.subplots(figsize=(14, fig_height))

row_h = 8
row_gap = 3

for i, sid in enumerate(stations_ordered):
    y = i * (row_h + row_gap)

    eps = ep[ep["Station ID"] == sid].sort_values("start")
    if eps.empty:
        continue

    segs = []
    for _, r in eps.iterrows():
        start_num = mdates.date2num(r["start"])
        segs.append((start_num, r["duration_days"]))
    ax.broken_barh(segs, (y, row_h))

yticks = [i * (row_h + row_gap) + row_h/2 for i in range(len(stations_ordered))]
ylabels = [station_label_map.get(sid, str(sid)) for sid in stations_ordered]
ax.set_yticks(yticks)
ax.set_yticklabels(ylabels)

x_min = pd.Timestamp("2020-01-01")
x_max = pd.Timestamp("2024-12-31")
ax.set_xlim(x_min, x_max)

ax.xaxis.set_major_locator(mdates.YearLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))
ax.xaxis.set_minor_locator(mdates.MonthLocator())

ax.set_xlabel("Date")
ax.set_ylabel("Station")
ax.set_title(f"Station-level PM2.5 Episodes (threshold={THRESH}, min_hours={MIN_HOURS}, min_len={MIN_LEN})")

ax.grid(True, axis="x", which="major", linestyle="-")
fig.autofmt_xdate()
plt.tight_layout()
plt.show()

"""## 3 Data Preprocessing

### 3.1 Daily Average Data
"""

import pandas as pd
import numpy as np

df = pm25_hourly_wide_final.copy()

hour_cols = [f"H{i:02d}" for i in range(1, 25)]
missing = [c for c in hour_cols if c not in df.columns]
if missing:
    raise ValueError(f"Missing hour columns: {missing}")

df["n_hours"] = df[hour_cols].notna().sum(axis=1)

df["pm25_daily_avg"] = df[hour_cols].mean(axis=1, skipna=True)

df.loc[df["n_hours"] == 0, "pm25_daily_avg"] = np.nan

df = df.drop(columns=hour_cols)

df.head()

"""### 3.2 Low Quality Data (n_hours < 15)"""

# we define low quality data as n_hours < 15
low = df[df["n_hours"] < 15].copy()

print(f"Rows with n_hours < 15: {len(low):,} / {len(df):,}")

print(low.to_string(index=False))

"""### 3.3 Translate Into CSV"""

df.to_csv("pm25_daily_cleaned.csv", index=False)

df.to_csv("/content/pm25_daily_cleaned.csv", index=False)